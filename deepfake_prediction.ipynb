{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./kfolddeepfakeeffb2-flip\n",
      "./models\n",
      "./test_videos\n",
      "./sample_submission.csv\n",
      "./face_detector\n",
      "./final-kfold-inference-effb2.ipynb\n",
      "./kdold-deepfake-effb2\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "\n",
    "# Input data files are available in the \"\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "for path in glob('./*'):\n",
    "    print(path)\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "from copy import deepcopy\n",
    "from typing import Union, List, Tuple, Optional, Callable\n",
    "from collections import OrderedDict, defaultdict\n",
    "import math\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torch.utils.data.sampler import SequentialSampler, RandomSampler\n",
    "from torchvision import transforms, models\n",
    "from torchvision.transforms import Normalize\n",
    "from tqdm import tqdm\n",
    "from sklearn.cluster import DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_H, TARGET_W = 224, 224\n",
    "FRAMES_PER_VIDEO = 30\n",
    "TEST_VIDEOS_PATH = 'test_videos'\n",
    "NN_MODEL_PATHS = [\n",
    "    'kdold-deepfake-effb2/fold0-effb2-000epoch.pt',\n",
    "    'kdold-deepfake-effb2/fold0-effb2-001epoch.pt',\n",
    "    'kdold-deepfake-effb2/fold0-effb2-002epoch.pt',\n",
    "    'kfolddeepfakeeffb2-flip/fold0-flip-effb2-000epoch.pt',\n",
    "    'kfolddeepfakeeffb2-flip/fold0-flip-effb2-001epoch.pt',\n",
    "    'kfolddeepfakeeffb2-flip/fold0-flip-effb2-002epoch.pt',\n",
    "    \n",
    "    'kdold-deepfake-effb2/fold1-effb2-000epoch.pt',\n",
    "    'kdold-deepfake-effb2/fold1-effb2-001epoch.pt',\n",
    "    'kdold-deepfake-effb2/fold1-effb2-002epoch.pt',\n",
    "    'kfolddeepfakeeffb2-flip/fold1-flip-effb2-000epoch.pt',\n",
    "    'kfolddeepfakeeffb2-flip/fold1-flip-effb2-001epoch.pt',\n",
    "    'kfolddeepfakeeffb2-flip/fold1-flip-effb2-002epoch.pt',\n",
    "    \n",
    "    'kdold-deepfake-effb2/fold2-effb2-000epoch.pt',\n",
    "    'kdold-deepfake-effb2/fold2-effb2-001epoch.pt',\n",
    "    'kdold-deepfake-effb2/fold2-effb2-002epoch.pt',\n",
    "    'kfolddeepfakeeffb2-flip/fold2-flip-effb2-000epoch.pt',\n",
    "    'kfolddeepfakeeffb2-flip/fold2-flip-effb2-001epoch.pt',\n",
    "    'kfolddeepfakeeffb2-flip/fold2-flip-effb2-002epoch.pt',\n",
    "\n",
    "    'kdold-deepfake-effb2/fold3-effb2-000epoch.pt',\n",
    "    'kdold-deepfake-effb2/fold3-effb2-001epoch.pt',\n",
    "    'kdold-deepfake-effb2/fold3-effb2-002epoch.pt',\n",
    "    'kfolddeepfakeeffb2-flip/fold3-flip-effb2-000epoch.pt',\n",
    "    'kfolddeepfakeeffb2-flip/fold3-flip-effb2-001epoch.pt',\n",
    "    'kfolddeepfakeeffb2-flip/fold3-flip-effb2-002epoch.pt',\n",
    "\n",
    "    'kdold-deepfake-effb2/fold4-effb2-000epoch.pt',\n",
    "    'kdold-deepfake-effb2/fold4-effb2-001epoch.pt',\n",
    "    'kdold-deepfake-effb2/fold4-effb2-002epoch.pt',\n",
    "    'kfolddeepfakeeffb2-flip/fold4-flip-effb2-000epoch.pt',\n",
    "    'kfolddeepfakeeffb2-flip/fold4-flip-effb2-001epoch.pt',\n",
    "    'kfolddeepfakeeffb2-flip/fold4-flip-effb2-002epoch.pt',\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 1.6.0+cu101\n",
      "CUDA version: 10.1\n",
      "cuDNN version: 7603\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"face-detector\")\n",
    "\n",
    "from face_detector import FaceDetector\n",
    "from face_detector.utils import VideoReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from efficientnet_pytorch import EfficientNet\n",
    "\n",
    "def get_net():\n",
    "    net = EfficientNet.from_name('efficientnet-b2')\n",
    "    net._fc = nn.Linear(in_features=net._fc.in_features, out_features=2, bias=True)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnboundVideo():\n",
    "    def __init__(self, video_path):\n",
    "        self.video_paths = video_path\n",
    "        self.filenames = 0\n",
    "        self.face_dr = FaceDetector(frames_per_video=FRAMES_PER_VIDEO)\n",
    "\n",
    "        mean = [0.485, 0.456, 0.406]\n",
    "        std = [0.229, 0.224, 0.225]\n",
    "        self.normalize_transform = Normalize(mean, std)\n",
    "        \n",
    "        self.video_reader = VideoReader()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.filenames.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        my_frames, my_idxs = self.video_reader.read_frames(self.video_paths, FRAMES_PER_VIDEO)\n",
    "        faces = self.face_dr.get_faces(\n",
    "            my_frames, my_idxs,\n",
    "            0.7, 0.7, 0.7, 0.6\n",
    "        )\n",
    "\n",
    "        n = len(faces)\n",
    "\n",
    "        video = torch.zeros((n, 3, TARGET_H, TARGET_W))\n",
    "        for i, face in enumerate(faces[:n]):\n",
    "            face = 255 - face\n",
    "            face = face.astype(np.float32)/255.\n",
    "            face = torch.tensor(face)\n",
    "            face = face.permute(2,0,1)\n",
    "            face = self.normalize_transform(face)\n",
    "            video[i] = face\n",
    "\n",
    "        return video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepFakePredictor:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.models = [self.prepare_model(get_net(), path) for path in NN_MODEL_PATHS]\n",
    "        self.models_count = len(self.models)\n",
    "\n",
    "    def predict(self, video): # Тут модель принимает на вход путь до видео\n",
    "        result = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            video = video.to(self.device, dtype=torch.float32)\n",
    "            try:\n",
    "                label = self.predict_ensemble(video)\n",
    "            except Exception as e:\n",
    "                print(f'Warning! {e}, {type(e)}')\n",
    "                label = 0.5\n",
    "\n",
    "        return label\n",
    "\n",
    "    def prepare_model(self, model, path):\n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        model.to(self.device);\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            model = model.cuda()\n",
    "            \n",
    "        if torch.cuda.is_available():\n",
    "            checkpoint = torch.load(path)\n",
    "        else:\n",
    "            checkpoint = torch.load(path, map_location=torch.device('cpu'))\n",
    "            \n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        model.eval()\n",
    "        print(f'Model prepared. Device is {self.device}')\n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def net_forward(net, inputs):\n",
    "        bs = inputs.size(0)\n",
    "        # Convolution layers\n",
    "        x = net.extract_features(inputs)\n",
    "        # Pooling and final linear layer\n",
    "        x = net._avg_pooling(x)\n",
    "        emb = x.view(bs, -1)\n",
    "        x = net._dropout(emb)\n",
    "        x = net._fc(x)\n",
    "        return emb, x\n",
    "    \n",
    "    def postprocess(self, embs, predictions):\n",
    "        clusters = defaultdict(list)\n",
    "        for prediction, cluster_id in zip(predictions, DBSCAN(eps=1.2, min_samples=1).fit_predict(embs)):\n",
    "            clusters[cluster_id].append(prediction)\n",
    "        sorted_clusters = sorted(clusters.items(), key=lambda x: -len(x[1]))\n",
    "        if len(sorted_clusters) < 2:\n",
    "            return sorted_clusters[0][1]\n",
    "        if len(sorted_clusters[1][1]) / len(predictions) > 0.25:\n",
    "            return sorted_clusters[0][1] + sorted_clusters[1][1]\n",
    "        return sorted_clusters[0][1]\n",
    "    \n",
    "    def predict_ensemble(self, video):\n",
    "        embs, predictions = 0, 0\n",
    "        for model in self.models:\n",
    "            emb, prediction = self.net_forward(model, video)\n",
    "            predictions += prediction / self.models_count\n",
    "            embs += emb / self.models_count\n",
    "\n",
    "        predictions = nn.functional.softmax(predictions, dim=1).data.cpu().numpy()[:,1]\n",
    "        embs = embs.cpu().numpy()\n",
    "        \n",
    "        predictions = self.postprocess(embs, predictions)\n",
    "        return np.mean(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model prepared. Device is cuda:0\n",
      "Model prepared. Device is cuda:0\n",
      "Model prepared. Device is cuda:0\n",
      "Model prepared. Device is cuda:0\n",
      "Model prepared. Device is cuda:0\n",
      "Model prepared. Device is cuda:0\n",
      "Model prepared. Device is cuda:0\n",
      "Model prepared. Device is cuda:0\n",
      "Model prepared. Device is cuda:0\n",
      "Model prepared. Device is cuda:0\n",
      "Model prepared. Device is cuda:0\n",
      "Model prepared. Device is cuda:0\n",
      "Model prepared. Device is cuda:0\n",
      "Model prepared. Device is cuda:0\n",
      "Model prepared. Device is cuda:0\n",
      "Model prepared. Device is cuda:0\n",
      "Model prepared. Device is cuda:0\n",
      "Model prepared. Device is cuda:0\n",
      "Model prepared. Device is cuda:0\n",
      "Model prepared. Device is cuda:0\n",
      "Model prepared. Device is cuda:0\n",
      "Model prepared. Device is cuda:0\n",
      "Model prepared. Device is cuda:0\n",
      "Model prepared. Device is cuda:0\n",
      "Model prepared. Device is cuda:0\n",
      "Model prepared. Device is cuda:0\n",
      "Model prepared. Device is cuda:0\n",
      "Model prepared. Device is cuda:0\n",
      "Model prepared. Device is cuda:0\n",
      "Model prepared. Device is cuda:0\n"
     ]
    }
   ],
   "source": [
    "deep_fake_predictor = DeepFakePredictor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "for video in glob('test_videos/*')[:5]:\n",
    "    result = deep_fake_predictor.predict(UnboundVideo(video).__getitem__(0))\n",
    "    results[video] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test_videos/zuwwbbusgl.mp4': 0.058104407,\n",
       " 'test_videos/zyufpqvpyu.mp4': 0.4631741,\n",
       " 'test_videos/ziipxxchai.mp4': 0.42018613,\n",
       " 'test_videos/zgbhzkditd.mp4': 0.9176976,\n",
       " 'test_videos/zgjosltkie.mp4': 0.91377324}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "user_python3",
   "language": "python",
   "name": "user_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
